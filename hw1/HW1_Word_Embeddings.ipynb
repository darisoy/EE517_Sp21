{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pretrained word2vec\n",
    "# Googleâ€™s pre-trained Word2Vec (1.5GB), word vectors for a vocabulary of 3 million words \n",
    "# and phrases that they trained on roughly 100 billion words from a Google News dataset\n",
    "# https://code.google.com/archive/p/word2vec/\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./word2vec_pretrained/GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  3000000\n"
     ]
    }
   ],
   "source": [
    "# Check vocab size\n",
    "words = [w for w in model.key_to_index]\n",
    "print(\"Vocab size: \", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine distance between two vectors\n",
    "def get_cosine_dist(vec1, vec2):\n",
    "    dist = np.dot(vec1, vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance (similar) : 0.8322346\n",
      "Cosine distance (different) : 0.14340287\n"
     ]
    }
   ],
   "source": [
    "# Sanity check on distance function\n",
    "a = model.__getitem__(\"Yangtze_River\")\n",
    "b = model.__getitem__(\"Yangtze\")\n",
    "c = model.__getitem__(\"sushi\")\n",
    "print(\"Cosine distance (similar) :\", get_cosine_dist(a, b))\n",
    "print(\"Cosine distance (different) :\", get_cosine_dist(a, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get k closest words for an embedding\n",
    "def get_neighbors(target_word_vec, vocab_words, k):\n",
    "    '''\n",
    "    Input:\n",
    "    target_word_vec: word embedding\n",
    "    vocab_words: list of words in vocabulary\n",
    "    k: number of neighbors to get\n",
    "    \n",
    "    Output:\n",
    "    k_nearest: k nearest words\n",
    "    '''\n",
    "    \n",
    "    # calculate cosine distance between target word embedding and each word embedding of words in vocab\n",
    "    # for each word in vocab, record cosine distance and that word \n",
    "    dist_item = [[None,None] for word in vocab_words]\n",
    "    for i in range(len(vocab_words)):\n",
    "        item = vocab_words[i]\n",
    "        item_word_vec =  model.__getitem__(item)\n",
    "        item_dist = get_cosine_dist(item_word_vec, target_word_vec)\n",
    "        dist_item[i] = [item_dist,item]\n",
    "\n",
    "    # now sort dist_item so words with the most positive cosine distance will be at the front of the list\n",
    "    sorted_dist_item = sorted(dist_item, reverse = True)\n",
    "\n",
    "    # get labels of k nearest neighbours\n",
    "    k_nearest = []\n",
    "    for neighbour in sorted_dist_item[:k]:\n",
    "        k_nearest.append(neighbour[1])\n",
    "\n",
    "    return k_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest10_for_analogy_list(filename):\n",
    "\n",
    "    word_list_file = open(filename).readlines()\n",
    "    analogies = [line.strip().split() for line in word_list_file if line.strip() != \"\"]\n",
    "    \n",
    "    closest_words_list = [None for i in range(len(analogies))]\n",
    "    answer = [item[-1] for item in analogies]\n",
    "    \n",
    "    for idx in tqdm(range(len(analogies))):\n",
    "        analogy = analogies[idx]\n",
    "        # look up embeddings for words a, b, c\n",
    "        vec_a = model.__getitem__(analogy[0])\n",
    "        vec_b = model.__getitem__(analogy[1])\n",
    "        vec_c = model.__getitem__(analogy[2])\n",
    "\n",
    "        # estimated embedding for word d\n",
    "        est_vec_d = vec_c + (vec_b - vec_a)\n",
    "\n",
    "        # find top 10 words in vocab\n",
    "        top10_closest_words = get_neighbors(est_vec_d, words, 10)\n",
    "        closest_words_list[idx] = top10_closest_words\n",
    "    \n",
    "    return closest_words_list, answer, analogies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze performance for the 4 lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f95a4ec8ec7473489c0e4eb110b420d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top similar words for file word_lists/list1.txt saved!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f423684d4aa94d68a8b706a9eb3504af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top similar words for file word_lists/list2.txt saved!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93f6bcbc7334f4b81485c7913f436d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top similar words for file word_lists/list3.txt saved!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebd57b02f9c40698dc368f25897159b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top similar words for file word_lists/list4.txt saved!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ec7959cfbd45558d936b2a5b5ea4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=40), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top similar words for file word_lists/list5.txt saved!\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    input_list = ('word_lists/list%d.txt' % (i+1))\n",
    "    closest_found_list, correct_list, analogies_list = find_closest10_for_analogy_list(input_list)\n",
    "    list_results = np.array([closest_found_list, correct_list, analogies_list], dtype=object)\n",
    "    with open(('list%d_similar10.data' % (i+1)), 'wb') as data_save:\n",
    "        # store the data as binary data stream\n",
    "        pickle.dump(list_results, data_save)\n",
    "    print(\"Top similar words for file \" + input_list + \" saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(closest_found, correct, analogies, repeat_orig = True):\n",
    "#     # lowercase all\n",
    "#     correct = [word.lower() for word in correct]\n",
    "#     analogies = [[word.lower() for word in analogy] for analogy in analogies]\n",
    "#     closest_found = [[word.lower() for word in top10] for top10 in closest_found]\n",
    "    \n",
    "    total = len(correct)\n",
    "    correct_cnt_top1 = 0\n",
    "    correct_cnt_top3 = 0\n",
    "    closest_found_new = [None for i in range(len(closest_found))]\n",
    "    if not repeat_orig:\n",
    "        for i in range(len(closest_found)):\n",
    "            closest_found_new[i] = np.array([el for el in closest_found[i] if el not in analogies[i][:3]]) \n",
    "    else:\n",
    "        closest_found_new = closest_found\n",
    "        \n",
    "    for i in range(len(correct)):\n",
    "        answer_word = correct[i]\n",
    "        top1 = closest_found_new[i][0]\n",
    "        top3 = closest_found_new[i][:3]\n",
    "        if answer_word == top1:\n",
    "            correct_cnt_top1 += 1\n",
    "        if answer_word in top3:\n",
    "            correct_cnt_top3 += 1\n",
    "#         else:\n",
    "#             print(\"Analogy :\", analogies[i])\n",
    "#             print(closest_found[i])\n",
    "#             print(closest_found_new[i])\n",
    "#             print(top1)\n",
    "#             print(top3)\n",
    "            \n",
    "    top1_acc = correct_cnt_top1/total\n",
    "    top3_acc = correct_cnt_top3/total\n",
    "    \n",
    "    return top1_acc, top3_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing file :  word_lists/list1.txt\n",
      "=== Without removing similar words found that are in words a,b,c ===\n",
      "Top1 accuracy rate for word list 1: 40.0 %\n",
      "Top3 accuracy rate for word list 1: 85.0 %\n",
      "=== After removing similar words found that are in words a,b,c ===\n",
      "Top1 accuracy rate for word list 1: 85.0 %\n",
      "Top3 accuracy rate for word list 1: 90.0 %\n",
      "============================================================\n",
      "Testing file :  word_lists/list2.txt\n",
      "=== Without removing similar words found that are in words a,b,c ===\n",
      "Top1 accuracy rate for word list 2: 35.0 %\n",
      "Top3 accuracy rate for word list 2: 90.0 %\n",
      "=== After removing similar words found that are in words a,b,c ===\n",
      "Top1 accuracy rate for word list 2: 85.0 %\n",
      "Top3 accuracy rate for word list 2: 90.0 %\n",
      "============================================================\n",
      "Testing file :  word_lists/list3.txt\n",
      "=== Without removing similar words found that are in words a,b,c ===\n",
      "Top1 accuracy rate for word list 3: 0.0 %\n",
      "Top3 accuracy rate for word list 3: 40.0 %\n",
      "=== After removing similar words found that are in words a,b,c ===\n",
      "Top1 accuracy rate for word list 3: 30.0 %\n",
      "Top3 accuracy rate for word list 3: 50.0 %\n",
      "============================================================\n",
      "Testing file :  word_lists/list4.txt\n",
      "=== Without removing similar words found that are in words a,b,c ===\n",
      "Top1 accuracy rate for word list 4: 0.0 %\n",
      "Top3 accuracy rate for word list 4: 15.0 %\n",
      "=== After removing similar words found that are in words a,b,c ===\n",
      "Top1 accuracy rate for word list 4: 15.0 %\n",
      "Top3 accuracy rate for word list 4: 20.0 %\n",
      "============================================================\n",
      "Testing file :  word_lists/list5.txt\n",
      "=== Without removing similar words found that are in words a,b,c ===\n",
      "Top1 accuracy rate for word list 5: 12.5 %\n",
      "Top3 accuracy rate for word list 5: 65.0 %\n",
      "=== After removing similar words found that are in words a,b,c ===\n",
      "Top1 accuracy rate for word list 5: 55.00000000000001 %\n",
      "Top3 accuracy rate for word list 5: 72.5 %\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Instead of taking a long time to go over the vocab, we load the pickle file we saved\n",
    "for i in range(5):\n",
    "    test_file = ('list%d_similar10.data' % (i+1))\n",
    "    print(\"Testing file : \", ('word_lists/list%d.txt' % (i+1)))\n",
    "    with open(test_file, 'rb') as filehandle:\n",
    "        # read the data as binary data stream\n",
    "        closest_found, correct, analogies = np.array(pickle.load(filehandle))\n",
    "        print(\"=== Without removing similar words found that are in words a,b,c ===\")\n",
    "        list_top1_acc, list_top3_acc = evaluate_accuracy(closest_found, correct, analogies)\n",
    "        print((\"Top1 accuracy rate for word list %d:\" % (i+1)) , list_top1_acc * 100, \"%\")\n",
    "        print((\"Top3 accuracy rate for word list %d:\" % (i+1)) , list_top3_acc * 100, \"%\")\n",
    "        print(\"=== After removing similar words found that are in words a,b,c ===\")\n",
    "        list_top1_acc, list_top3_acc = evaluate_accuracy(closest_found, correct, analogies, repeat_orig = False)\n",
    "        print((\"Top1 accuracy rate for word list %d:\" % (i+1)) , list_top1_acc * 100, \"%\")\n",
    "        print((\"Top3 accuracy rate for word list %d:\" % (i+1)) , list_top3_acc * 100, \"%\")\n",
    "        print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small example of another type of analogy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up embeddings for words a, b, c\n",
    "vec_a = model.__getitem__(\"Tokyo\")\n",
    "vec_b = model.__getitem__(\"Japan\")\n",
    "vec_c = model.__getitem__(\"Paris\")\n",
    "\n",
    "# estimated embedding for word d\n",
    "est_vec_d = vec_c + (vec_b - vec_a)\n",
    "\n",
    "# find top 3 words in vocab\n",
    "top3_closest_words = get_neighbors(est_vec_d, words, 3)\n",
    "print(top3_closest_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
