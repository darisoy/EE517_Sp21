{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pretrained word2vec\n",
    "# Googleâ€™s pre-trained Word2Vec (1.5GB), word vectors for a vocabulary of 3 million words \n",
    "# and phrases that they trained on roughly 100 billion words from a Google News dataset\n",
    "# https://code.google.com/archive/p/word2vec/\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./word2vec_pretrained/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size:  3000000\n"
     ]
    }
   ],
   "source": [
    "# Check vocab size\n",
    "words = [w for w in model.key_to_index]\n",
    "print(\"Vocab size: \", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_closest_for_analogy(filename, n):\n",
    "    word_list_file = open(filename).readlines()\n",
    "    analogies = [line.strip().split() for line in word_list_file if line.strip() != \"\"]\n",
    "    \n",
    "    closest_words_list = [None for i in range(len(analogies))]\n",
    "    answer = [item[3] for item in analogies]\n",
    "    \n",
    "    for idx, analogy in enumerate(analogies):\n",
    "        top3_similar_words = model.most_similar(positive=[analogy[1], analogy[2]], \n",
    "                                                negative=[analogy[0]], topn=n)\n",
    "        closest_words_list[idx] = [w for (w, _) in top3_similar_words]\n",
    "    \n",
    "    return closest_words_list, answer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze performance for the 4 lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(closest_found, correct, n=3):\n",
    "    top1 = np.array([c[0] for c in closest_found]) == np.array(correct)\n",
    "    top3 = top1\n",
    "    for i in range(1, n):\n",
    "        top3 = np.logical_or(top3, np.array([c[i] for c in closest_found]) == np.array(correct))\n",
    "    return np.sum(top1)/len(correct), np.sum(top3)/len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_analogies_for_list(filename):\n",
    "    print('Testing file: ', filename)\n",
    "    closest_found, correct = find_n_closest_for_analogy(filename, 3)\n",
    "    top1_acc, top3_acc = evaluate_accuracy(closest_found, correct)\n",
    "    print(\"     Top1 accuracy rate for word list 1:\", top1_acc * 100, \"%\")\n",
    "    print(\"     Top3 accuracy rate for word list 1:\", top3_acc * 100, \"%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing file:  word_lists/list1.txt\n",
      "     Top1 accuracy rate for word list 1: 85.0 %\n",
      "     Top3 accuracy rate for word list 1: 90.0 %\n",
      "\n",
      "Testing file:  word_lists/list2.txt\n",
      "     Top1 accuracy rate for word list 1: 85.0 %\n",
      "     Top3 accuracy rate for word list 1: 90.0 %\n",
      "\n",
      "Testing file:  word_lists/list3.txt\n",
      "     Top1 accuracy rate for word list 1: 35.0 %\n",
      "     Top3 accuracy rate for word list 1: 45.0 %\n",
      "\n",
      "Testing file:  word_lists/list4.txt\n",
      "     Top1 accuracy rate for word list 1: 20.0 %\n",
      "     Top3 accuracy rate for word list 1: 25.0 %\n",
      "\n",
      "Testing file:  word_lists/list5.txt\n",
      "     Top1 accuracy rate for word list 1: 58.108108108108105 %\n",
      "     Top3 accuracy rate for word list 1: 68.91891891891892 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    predict_analogies_for_list(('word_lists/list%d.txt' % (i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd0ba9fc0d24ec5b5a9345c7118634a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3000000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['France', 'Paris', 'Belgium']\n"
     ]
    }
   ],
   "source": [
    "# look up embeddings for words a, b, c\n",
    "vec_a = model.__getitem__(\"Tokyo\")\n",
    "vec_b = model.__getitem__(\"Japan\")\n",
    "vec_c = model.__getitem__(\"Paris\")\n",
    "\n",
    "# estimated embedding for word d\n",
    "est_vec_d = vec_c + (vec_b - vec_a)\n",
    "\n",
    "# find top 3 words in vocab\n",
    "top3_closest_words = get_neighbors(est_vec_d, words, 3)\n",
    "print(top3_closest_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd05b137523260739573695466c60f5bbef4a5b6a13fbab04d213a4e80cd7778902",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}