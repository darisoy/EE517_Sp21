{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "EE517: HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darisoy/EE517_Sp21/blob/master/hw3/hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXSx0jEDC_Kg"
      },
      "source": [
        "# Setup Python environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzsOPfQRQGfa",
        "outputId": "ee4ec091-6461-4b02-ae39-6dc2807363cb"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKHw2uxSPksV"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7EpasiPPksV"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiwHKnN7u_MH"
      },
      "source": [
        "labels = {'O' : 0,\n",
        "          'B-geo-loc' : 1,\n",
        "          'I-geo-loc' : 2,\n",
        "          'B-product' : 3,\n",
        "          'I-product' : 4,\n",
        "          'B-facility' : 5,\n",
        "          'I-facility' : 6,\n",
        "          'B-company' : 7,\n",
        "          'I-company' : 8,\n",
        "          'B-person' : 9,\n",
        "          'I-person' : 10,\n",
        "          'B-sportsteam' : 11,\n",
        "          'I-sportsteam' : 12,\n",
        "          'B-musicartist' : 13,\n",
        "          'I-musicartist' : 14,\n",
        "          'B-movie' : 15,\n",
        "          'I-movie' : 16,\n",
        "          'B-tvshow' : 17,\n",
        "          'I-tvshow' : 18,\n",
        "          'B-other' : 19,\n",
        "          'I-other' : 20,\n",
        "          }\n",
        "end_token = '<END>'\n",
        "beg_token = '<BEG>'"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H-4YqDYTTeG"
      },
      "source": [
        "def get_sentences(df):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    running_sentence = [beg_token]\n",
        "    runnnig_label = [0]\n",
        "    for idx, row in df.iterrows():\n",
        "        running_sentence.append(row.word)\n",
        "        runnnig_label.append(row.tag)\n",
        "        if row.word == end_token:\n",
        "            sentences.append(running_sentence)\n",
        "            labels.append(runnnig_label)\n",
        "            running_sentence = [beg_token]\n",
        "            runnnig_label = [0]\n",
        "    return sentences, labels\n",
        "\n",
        "def get_data(type):\n",
        "    data = pd.read_csv('https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/data/' + type, delimiter='\\t', names=[\"word\", \"tag\"], skip_blank_lines=False, quoting=3)\n",
        "    data = data.fillna({'word': end_token, 'tag': 'O'})\n",
        "    data.tag = data.tag.apply((lambda x: labels[x]))\n",
        "    return get_sentences(data)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHgdBEuSPfhr"
      },
      "source": [
        "# Encode the data using BERT transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHxHvquJPksX"
      },
      "source": [
        "## Load the transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLkUmk7MPksY"
      },
      "source": [
        "transformer_name = \"distilbert-base-uncased\"\n",
        "transformer = DistilBertModel.from_pretrained(transformer_name)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(transformer_name)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7-zKa87PksY"
      },
      "source": [
        "## Preprocessing before sending to the transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csXlJsQ9BSkw"
      },
      "source": [
        "def get_BERT_loader(dataset, max_len=182, batch_size=64):\n",
        "    sentences, labels = get_data(dataset)\n",
        "    token = [[tokenizer.encode(w, add_special_tokens=True) for w in s] for s in sentences]\n",
        "    flat_token = [list(np.concatenate(sentence).flat) for sentence in token]\n",
        "    padded = np.array([i + [0]*(max_len-len(i)) for i in flat_token])\n",
        "    attention_mask = np.where(padded != 0, 1, 0)\n",
        "    data = TensorDataset(torch.from_numpy(padded), torch.from_numpy(attention_mask))\n",
        "    data_loader = DataLoader(data, shuffle=False, batch_size=batch_size)\n",
        "    return data_loader, labels"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F807xwX6S43x"
      },
      "source": [
        "train_trans_loader, train_labels = get_BERT_loader('train')\n",
        "valid_trans_loader, valid_labels = get_BERT_loader('dev')\n",
        "test_trans_loader, test_labels = get_BERT_loader('test')"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-7aohfNPksa"
      },
      "source": [
        "## Encode Using BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmtU-RqtPksa"
      },
      "source": [
        "def get_embeddings(target_loader):\n",
        "    transformer.eval()\n",
        "    transformer.to(device)\n",
        "    all_embed = []\n",
        "    for sample, sample_mask in tqdm(target_loader):\n",
        "        with torch.no_grad():\n",
        "            sample, sample_mask = sample.to(device), sample_mask.to(device)\n",
        "            last_hidden_states = transformer(sample, attention_mask=sample_mask)\n",
        "            all_embed.append(last_hidden_states[0][:,0,:])\n",
        "    return torch.cat(all_embed,dim =0)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8queiso6Srq"
      },
      "source": [
        "train_embed = get_embeddings(train_trans_loader)\n",
        "valid_embed = get_embeddings(valid_trans_loader)\n",
        "test_embed = get_embeddings(test_trans_loader)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25pSvH_3H5oc"
      },
      "source": [
        "def clean_embed_output(embed, token):\n",
        "    data_clean_embed = []\n",
        "    for i, token_sentence in enumerate(token):\n",
        "        clean_embed = []\n",
        "        embed_idx = 0\n",
        "        for word in token_sentence:\n",
        "            clean_embed.append(torch.mean(embed[i][embed_idx:embed_idx+len(word)]).item())\n",
        "            embed_idx += len(word)\n",
        "        data_clean_embed.append(torch.tensor(clean_embed[1:len(clean_embed)-1]))\n",
        "    return data_clean_embed"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv4cHI4tL4Rq"
      },
      "source": [
        "train_clean_embed = clean_embed_output(train_embed, train_token)\n",
        "valid_clean_embed = clean_embed_output(valid_embed, valid_token)\n",
        "test_clean_embed = clean_embed_output(test_embed, test_token)"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xui6Y2JkNvKf"
      },
      "source": [
        "train_labels_clean = [torch.tensor(x[1:len(x)-1]) for x in train_labels]\n",
        "valid_labels_clean = [torch.tensor(x[1:len(x)-1]) for x in valid_labels]\n",
        "test_labels_clean = [torch.tensor(x[1:len(x)-1]) for x in test_labels]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFI0bfCfPrz6"
      },
      "source": [
        "# Classify the embeddings using RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9uu3klIPwWq"
      },
      "source": [
        "# Model Definition\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNN, self).__init__()\n",
        "        self.rnn_layer = nn.RNN(input_size=1, hidden_size=128)\n",
        "        self.out_layer = nn.Linear(in_features=128, out_features=21)\n",
        "\n",
        "    def forward(self, sentence, h=None):\n",
        "        out = []\n",
        "        X_in = torch.unsqueeze(seq[0],0)\n",
        "        for X in sentence:\n",
        "            X_in = X.unsqueeze(dim = 0)\n",
        "            tmp, h = self.rnn_layer(X_in, h)\n",
        "            out.append(self.out_layer(tmp))\n",
        "        return torch.stack(out).squeeze(1), h"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq8o5BE0QJgV"
      },
      "source": [
        "# train the classifier NOT TESTED YET\n",
        "classifier = RNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(seq.parameters(), lr=0.001)\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, inputs in enumerate(tqdm(train_clean_embed)):\n",
        "        labels = train_labels_clean[i]\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = classifier(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print('[Epoch %d]\\tTrain Loss: \\t\\t%.3f' % (epoch+1, running_loss / len(train_loader)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}