{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "EE517: HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darisoy/EE517_Sp21/blob/master/hw3/hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzsOPfQRQGfa"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKHw2uxSPksV"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7EpasiPPksV"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiwHKnN7u_MH"
      },
      "source": [
        "labels = {'O' : 0,\n",
        "          'B-geo-loc' : 1,\n",
        "          'I-geo-loc' : 2,\n",
        "          'B-product' : 3,\n",
        "          'I-product' : 4,\n",
        "          'B-facility' : 5,\n",
        "          'I-facility' : 6,\n",
        "          'B-company' : 7,\n",
        "          'I-company' : 8,\n",
        "          'B-person' : 9,\n",
        "          'I-person' : 10,\n",
        "          'B-sportsteam' : 11,\n",
        "          'I-sportsteam' : 12,\n",
        "          'B-musicartist' : 13,\n",
        "          'I-musicartist' : 14,\n",
        "          'B-movie' : 15,\n",
        "          'I-movie' : 16,\n",
        "          'B-tvshow' : 17,\n",
        "          'I-tvshow' : 18,\n",
        "          'B-other' : 19,\n",
        "          'I-other' : 20,\n",
        "          }\n",
        "end_token = '<END>'\n",
        "beg_token = '<BEG>'"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H-4YqDYTTeG"
      },
      "source": [
        "def get_sentences(df):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    running_sentence = [beg_token]\n",
        "    runnnig_label = [0]\n",
        "    for idx, row in df.iterrows():\n",
        "        running_sentence.append(row.word)\n",
        "        runnnig_label.append(row.tag)\n",
        "        if row.word == end_token:\n",
        "            sentences.append(running_sentence)\n",
        "            labels.append(runnnig_label)\n",
        "\n",
        "            running_sentence = [beg_token]\n",
        "            runnnig_label = [0]\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "def get_data(type):\n",
        "    data = pd.read_csv('https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/data/' + type, delimiter='\\t', names=[\"word\", \"tag\"], skip_blank_lines=False, quoting=3)\n",
        "    data = data.fillna({'word': end_token, 'tag': 'O'})\n",
        "    data.tag = data.tag.apply((lambda x: labels[x]))\n",
        "    return get_sentences(data)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMjqPZWDPksW"
      },
      "source": [
        "train_sentences, train_labels = get_data('train')\n",
        "valid_sentences, valid_labels = get_data('dev')\n",
        "test_sentences, test_labels = get_data('test')"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHgdBEuSPfhr"
      },
      "source": [
        "# Encode the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHxHvquJPksX"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLkUmk7MPksY"
      },
      "source": [
        "model_name = \"distilbert-base-uncased\"\n",
        "model = DistilBertModel.from_pretrained(model_name)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7-zKa87PksY"
      },
      "source": [
        "## Preprocessing before sending to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVMiI8XyPksZ"
      },
      "source": [
        "# This turns every word into the list of ids\n",
        "train_token = [[tokenizer.encode(w, add_special_tokens=True) for w in sentence] for sentence in train_sentences]\n",
        "valid_token = [[tokenizer.encode(w, add_special_tokens=True) for w in sentence] for sentence in valid_sentences]\n",
        "test_token = [[tokenizer.encode(w, add_special_tokens=True) for w in sentence] for sentence in test_sentences]"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIT1WigDl9u9",
        "outputId": "2deb2efc-c50b-456f-d8c3-fd1d88e21351"
      },
      "source": [
        "train_token[0]"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[101, 1026, 11693, 1028, 102],\n",
              " [101, 1030, 3520, 9856, 27610, 25855, 2213, 102],\n",
              " [101, 1030, 1056, 2290, 10790, 2581, 2620, 2487, 102],\n",
              " [101, 2027, 102],\n",
              " [101, 2097, 102],\n",
              " [101, 2022, 102],\n",
              " [101, 2035, 102],\n",
              " [101, 2589, 102],\n",
              " [101, 2011, 102],\n",
              " [101, 4465, 102],\n",
              " [101, 3404, 102],\n",
              " [101, 2033, 102],\n",
              " [101, 1008, 16837, 1008, 102],\n",
              " [101, 1026, 2203, 1028, 102]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-QslSMmE-cH"
      },
      "source": [
        "flat_train_token = [list(np.concatenate(sentence).flat) for sentence in train_token]\n",
        "flat_valid_token = [list(np.concatenate(sentence).flat) for sentence in valid_token]\n",
        "flat_test_token = [list(np.concatenate(sentence).flat) for sentence in test_token]"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH8POHy5PksY"
      },
      "source": [
        "def get_max_len(tokenized):\n",
        "    max_len = 0\n",
        "    for i in tokenized:\n",
        "        if len(i) > max_len:\n",
        "            max_len = len(i)\n",
        "    return max_len"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guRmd6pFPksZ",
        "outputId": "2f27f7ab-75d3-4ff1-ee46-ab86782a6e92"
      },
      "source": [
        "# Padding\n",
        "max_len = max(get_max_len(flat_train_token), get_max_len(flat_valid_token), get_max_len(flat_test_token))\n",
        "\n",
        "padded_train = np.array([i + [0]*(max_len-len(i)) for i in flat_train_token])\n",
        "padded_valid = np.array([i + [0]*(max_len-len(i)) for i in flat_valid_token])\n",
        "padded_test = np.array([i + [0]*(max_len-len(i)) for i in flat_test_token])\n",
        "\n",
        "print(\"Padded shape (train): \", np.array(padded_train).shape)\n",
        "print(\"Padded shape (valid): \", np.array(padded_valid).shape)\n",
        "print(\"Padded shape (test): \", np.array(padded_test).shape)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Padded shape (train):  (2394, 182)\n",
            "Padded shape (valid):  (1005, 182)\n",
            "Padded shape (test):  (3878, 182)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwy7aonzPksZ"
      },
      "source": [
        "# Tell BERT to ignore padding\n",
        "attention_mask_train = np.where(padded_train != 0, 1, 0)\n",
        "attention_mask_valid = np.where(padded_valid != 0, 1, 0)\n",
        "attention_mask_test = np.where(padded_test != 0, 1, 0)"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F807xwX6S43x"
      },
      "source": [
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(padded_train), torch.from_numpy(attention_mask_train))\n",
        "valid_data = TensorDataset(torch.from_numpy(padded_valid), torch.from_numpy(attention_mask_valid))\n",
        "test_data = TensorDataset(torch.from_numpy(padded_test), torch.from_numpy(attention_mask_test))\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZHWqCPdTBH2",
        "outputId": "24b40fc6-a00d-4a80-ed55-656378eb3cb5"
      },
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_mask_x = dataiter.next()\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample input mask size: ', sample_mask_x.size()) # batch_size, seq_length\n",
        "print('Sample input mask: \\n', sample_mask_x)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([64, 182])\n",
            "Sample input: \n",
            " tensor([[  101,  1026, 11693,  ...,     0,     0,     0],\n",
            "        [  101,  1026, 11693,  ...,     0,     0,     0],\n",
            "        [  101,  1026, 11693,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  1026, 11693,  ...,     0,     0,     0],\n",
            "        [  101,  1026, 11693,  ...,     0,     0,     0],\n",
            "        [  101,  1026, 11693,  ...,     0,     0,     0]])\n",
            "\n",
            "Sample input mask size:  torch.Size([64, 182])\n",
            "Sample input mask: \n",
            " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-7aohfNPksa"
      },
      "source": [
        "## Using BERT to encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmtU-RqtPksa"
      },
      "source": [
        "def get_embeddings(target_loader):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    all_embed = []\n",
        "    for sample, sample_mask in target_loader:\n",
        "        with torch.no_grad():\n",
        "            sample, sample_mask = sample.to(device), sample_mask.to(device)\n",
        "            last_hidden_states = model(sample, attention_mask=sample_mask)\n",
        "            all_embed.append(last_hidden_states[0][:,0,:])\n",
        "    return torch.cat(all_embed,dim =0)"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8queiso6Srq"
      },
      "source": [
        "train_embed = get_embeddings(train_loader)\n",
        "valid_embed = get_embeddings(valid_loader)\n",
        "test_embed = get_embeddings(test_loader)"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25pSvH_3H5oc"
      },
      "source": [
        "def clean_embed_output(embed, token):\n",
        "    data_clean_embed = []\n",
        "    for i, token_sentence in enumerate(token):\n",
        "        clean_embed = []\n",
        "        embed_idx = 0\n",
        "        for word in token_sentence:\n",
        "            clean_embed.append(torch.mean(embed[i][embed_idx:embed_idx+len(word)]).item())\n",
        "            embed_idx += len(word)\n",
        "        data_clean_embed.append(clean_embed[1:len(clean_embed)-1])\n",
        "    return data_clean_embed"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv4cHI4tL4Rq"
      },
      "source": [
        "train_clean_embed = clean_embed_output(train_embed, train_token)\n",
        "valid_clean_embed = clean_embed_output(valid_embed, valid_token)\n",
        "test_clean_embed = clean_embed_output(test_embed, test_token)"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDEOVNd2Pksb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef1e593-5506-485d-df7c-40af49dcc7f4"
      },
      "source": [
        "print('embedded train sentences: ', len(train_clean_embed))\n",
        "print('embedded valid sentences: ', len(valid_clean_embed))\n",
        "print('embedded test sentences: ', len(test_clean_embed))"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedded train sentences:  2394\n",
            "embedded valid sentences:  1005\n",
            "embedded test sentences:  3878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xui6Y2JkNvKf"
      },
      "source": [
        "train_labels_clean = [x[1:len(x)-1] for x in train_labels]\n",
        "valid_labels_clean = [x[1:len(x)-1] for x in valid_labels]\n",
        "test_labels_clean = [x[1:len(x)-1] for x in test_labels]"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5bEBrYoqMHZ"
      },
      "source": [
        "bert_embeddings = [train_clean_embed, train_labels_clean, valid_clean_embed, valid_labels_clean, test_clean_embed, test_labels_clean]"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFI0bfCfPrz6"
      },
      "source": [
        "# Classify the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9uu3klIPwWq"
      },
      "source": [
        "# Model Definition\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, p = 0.5):\n",
        "        super(RNN, self).__init__()\n",
        "        self.rnn_layer = nn.RNN(input_size=1, hidden_size=128)\n",
        "        self.out_layer = nn.Linear(in_features=128, out_features=21)\n",
        "        self.p = p #Whether to use actual seq or output for next step\n",
        "\n",
        "    def forward(self,seq, h = None):\n",
        "        out = []\n",
        "        X_in = torch.unsqueeze(seq[0],0)\n",
        "        for X in seq:\n",
        "            if np.random.rand()>self.p: #Use teacher forcing\n",
        "                X_in = X.unsqueeze(dim = 0)\n",
        "            tmp, h = self.rnn_layer(X_in, h)\n",
        "            X_in = self.out_layer(tmp)\n",
        "            out.append(X_in)\n",
        "        return torch.stack(out).squeeze(1), h"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq8o5BE0QJgV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}