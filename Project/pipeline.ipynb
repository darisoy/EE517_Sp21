{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "EE 517: Project pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darisoy/EE517_Sp21/blob/master/Project/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eQCgMJ3HVCz"
      },
      "source": [
        "# Set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mek_yYjHDLG"
      },
      "source": [
        "# install packages\n",
        "%%capture\n",
        "!pip install allennlp\n",
        "!pip install allennlp-models\n",
        "!pip install spacy-dbpedia-spotlight\n",
        "!pip install gender-guesser\n",
        "\n",
        "import spacy\n",
        "from allennlp.predictors.predictor import Predictor\n",
        "import spacy_dbpedia_spotlight\n",
        "import gender_guesser.detector as gender"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwffFN3Icgrf"
      },
      "source": [
        "# initialize models\n",
        "%%capture\n",
        "!python -m spacy download en_core_web_lg\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "allen_model_url = 'https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz'\n",
        "allen_predictor = Predictor.from_path(allen_model_url)  # load the model\n",
        "nel = spacy_dbpedia_spotlight.create('en')\n",
        "genDec = gender.Detector()"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao4OtUgPp3Dw"
      },
      "source": [
        "#list of person mentions that are not names\n",
        "fem_p = ['she', 'her', 'hers', 'herself']\n",
        "male_p = ['he', 'him', 'his', 'himself']\n",
        "personal_p = ['i', 'me', 'we', 'us', 'myself', 'ourself', 'ourselves']\n",
        "other_p = ['they', 'them', 'their', 'you', 'themself', 'themselves']\n",
        "people = ['adult','adults', 'person','people','child','children']\n",
        "\n",
        "person_f_singular = ['girl','woman','mrs','ms','mother','mom','aunt','niece','sister','wife','daughter','grandmother','grandma','grandmom','granddaughter','bride','girlfriend','gal','madam','lady']\n",
        "person_m_singular = ['boy','man','mr','father','dad','uncle','nephew','brother','husband','son','grandfather','grandpa','granddad','grandson','groom','boyfriend','guy','gentleman','bachelor']\n",
        "people_f_plural = ['girls','women','mothers','moms','aunts','nieces','sisters','wives','daughters','grandmothers','grandmas','granddaughters','brides','girlfriends','gals','ladies']\n",
        "people_m_plural = ['boys','men','fathers','dads','uncles','nephews','brothers','husbands','sons','grandfathers','grandpas','grandsons','grooms','boyfriends','guys','gentlemen','bachelors']\n",
        "people_f = person_f_singular + people_f_plural\n",
        "people_m = person_m_singular + people_m_plural\n",
        "\n",
        "un_named_mentions = fem_p + male_p + personal_p + other_p + people + people_f + people_m\n",
        "f_un_named = fem_p + people_f\n",
        "m_un_named = male_p + people_m\n",
        "neutral_un_named = personal_p + other_p + people"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPj_y-xBUnUx"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcGLEDtMVHJ0"
      },
      "source": [
        "# run text through models\n",
        "\n",
        "def get_coref(text, predictor):\n",
        "    prediction = predictor.predict(document=text)\n",
        "    return prediction['clusters'], prediction['document']\n",
        "\n",
        "def get_ner(text, nlp):\n",
        "    return [[ent.start, ent.end-1] for ent in nlp(text).ents if ent.label_ == 'PERSON']\n",
        "\n",
        "def get_nel(text, nel):\n",
        "    threshold = 0.95\n",
        "    return [[ent.start, ent.end-1] for ent in nel(text).ents if float(ent._.dbpedia_raw_result['@similarityScore']) >= threshold]\n",
        "\n",
        "def get_gender(person_mention):\n",
        "    if person_mention.lower() in un_named_mentions:\n",
        "        if person_mention.lower() in f_un_named:\n",
        "            return 'F'\n",
        "        elif person_mention.lower() in m_un_named:\n",
        "            return 'M'\n",
        "    else:\n",
        "        gen_name = genDec.get_gender(person_mention.capitalize())\n",
        "        if 'female' in gen_name:\n",
        "            return 'F'\n",
        "        elif 'male' in gen_name:\n",
        "            return 'M'\n",
        "    return '-'\n",
        "\n",
        "def get_pronouns(dic):\n",
        "    result = []\n",
        "    for i, word in enumerate(dic):\n",
        "        if word.lower() in un_named_mentions:\n",
        "            result.append([i, i])\n",
        "    return result"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsOV3OBUnjZZ"
      },
      "source": [
        "# converting spans to text\n",
        "\n",
        "def span_to_string(span, dic):\n",
        "    [a, b] = span\n",
        "    return \" \".join(dic[a:b+1])\n",
        "\n",
        "def array_to_text(array, dic):\n",
        "    return [span_to_string(e, dic) for e in array]\n",
        "\n",
        "def array2d_to_text(array2D, dic):\n",
        "    return [array_to_text(arr, dic) for arr in array2D]"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDhS0Ov5faHy"
      },
      "source": [
        "# get hypo & histo from model results\n",
        "\n",
        "def get_cluster(person, clusters):\n",
        "    for i, cluster in enumerate(clusters):\n",
        "        if person in cluster:\n",
        "            return clusters[i]\n",
        "    return [person]\n",
        "\n",
        "def split_hist_hypo(clusters, person_mentions, famous_people, gender):\n",
        "    historical = []\n",
        "    hypothetical = []\n",
        "    for i, person in enumerate(person_mentions):\n",
        "        # skip if not gendered\n",
        "        if gender[i] == '-':\n",
        "            continue\n",
        "        # skip if already in one of the lists\n",
        "        in_historical = any(person in sublist for sublist in historical)\n",
        "        in_hypothetical = any(person in sublist for sublist in hypothetical)\n",
        "        if in_historical or in_hypothetical:\n",
        "            continue\n",
        "        # add the cluster to correct list\n",
        "        person_set = get_cluster(person, clusters)\n",
        "        if person in famous_people:\n",
        "            historical.append(person_set)\n",
        "        else:\n",
        "            hypothetical.append(person_set)\n",
        "    return historical, hypothetical"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dIb0xPzZcZK"
      },
      "source": [
        "# pipline main function\n",
        "def get_hist_hypo_references(text, nlp, nel, debug=False):\n",
        "    doc = nlp(text)\n",
        "    clusters, dic = get_coref(text, allen_predictor)\n",
        "    person_mentions = get_ner(text, nlp) + get_pronouns(dic)\n",
        "    gender = [get_gender(span_to_string(p, dic)) for p in person_mentions]\n",
        "    famous_people = get_nel(text, nel)\n",
        "    hist, hypo = split_hist_hypo(clusters, person_mentions, famous_people, gender)\n",
        "    hist = array2d_to_text(hist, dic)\n",
        "    hypo = array2d_to_text(hypo, dic)\n",
        "    if debug and len(person_mentions) > 0:\n",
        "        print('***NEW SENTENCE***\\t\\t %s' % text)\n",
        "        print()\n",
        "        print('Coreference Model Output\\t %s' % array2d_to_text(clusters, dic))\n",
        "        print('NER + Pronouns Output\\t\\t %s' % array_to_text(person_mentions, dic))\n",
        "        print('Gender Guesser Output\\t\\t %s' % gender)\n",
        "        print('NEL Output\\t\\t\\t %s' % array_to_text(famous_people, dic,))\n",
        "        print()\n",
        "        print('Historical references\\t\\t %s' % hist)\n",
        "        print('Hypothetical references\\t\\t %s' % hypo)\n",
        "        print()\n",
        "        print()\n",
        "        print()\n",
        "    return hist, hypo"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehhkjavYGO5r",
        "outputId": "62b83423-e0d6-46ae-cca8-d211fec07b42"
      },
      "source": [
        "sample = 'Eva and Martha didn\\'t want their friend Jenny to feel lonely so they invited her to the party. Tom is happy.'\n",
        "hist, hypo = get_hist_hypo_references(sample, nlp, nel, debug=True)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***NEW SENTENCE***\t\t Eva and Martha didn't want their friend Jenny to feel lonely so they invited her to the party. Tom is happy.\n",
            "\n",
            "Coreference Model Output\t [['Eva and Martha', 'their', 'they'], ['their friend Jenny', 'her']]\n",
            "NER + Pronouns Output\t\t ['Eva', 'Martha', 'Jenny', 'Tom', 'their', 'they', 'her']\n",
            "Gender Guesser Output\t\t ['F', 'F', 'F', 'M', '-', '-', 'F']\n",
            "NEL Output\t\t\t []\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t [['Eva'], ['Martha'], ['Jenny'], ['Tom'], ['their friend Jenny', 'her']]\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9B1orA3G6NF"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB2dutoaHroM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "54d0b74a-a74d-4e57-e33f-34e647dd6997"
      },
      "source": [
        "import pandas as pd\n",
        "d_all1 = pd.read_csv('/content/train.csv', delimiter= \",\", low_memory=False, index_col=0)\n",
        "d_all2 = pd.read_csv('/content/test.csv', delimiter= \",\", low_memory=False, index_col=0)\n",
        "d_all1 = d_all1.drop(['bool'], axis=1)\n",
        "assert all(d_all1.columns == d_all2.columns)\n",
        "d = pd.concat([d_all1, d_all2], axis = 0)\n",
        "d.fillna('[]',inplace = True)\n",
        "d[d.grade=='12'].head()"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>book</th>\n",
              "      <th>grade</th>\n",
              "      <th>level</th>\n",
              "      <th>science</th>\n",
              "      <th>text</th>\n",
              "      <th>text_org</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Gr12_PhysicalSciences_Learner_Eng.txt3</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>sponsor this textbook was developed with corp...</td>\n",
              "      <td>SPONSOR This textbook was developed with corp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Gr12_PhysicalSciences_Learner_Eng.txt3</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>well structured , impactful corporate social ...</td>\n",
              "      <td>Well structured, impactful Corporate Social I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Gr12_PhysicalSciences_Learner_Eng.txt3</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>the merger between metropolitan and momentum ...</td>\n",
              "      <td>The merger between Metropolitan and Momentum ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Gr12_PhysicalSciences_Learner_Eng.txt3</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>hiv/aids is becoming a manageable disease in ...</td>\n",
              "      <td>HIV/AIDS is becoming a manageable disease in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Gr12_PhysicalSciences_Learner_Eng.txt3</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>momentum 's focus on persons with disabilitie...</td>\n",
              "      <td>Momentum's focus on persons with disabilities...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     book  ...                                           text_org\n",
              "5  Gr12_PhysicalSciences_Learner_Eng.txt3  ...   SPONSOR This textbook was developed with corp...\n",
              "6  Gr12_PhysicalSciences_Learner_Eng.txt3  ...   Well structured, impactful Corporate Social I...\n",
              "7  Gr12_PhysicalSciences_Learner_Eng.txt3  ...   The merger between Metropolitan and Momentum ...\n",
              "8  Gr12_PhysicalSciences_Learner_Eng.txt3  ...   HIV/AIDS is becoming a manageable disease in ...\n",
              "9  Gr12_PhysicalSciences_Learner_Eng.txt3  ...   Momentum's focus on persons with disabilities...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bjZfesUaMcPN",
        "outputId": "fdfca2f7-f8d1-43e3-ff30-b9fe2fc4e073"
      },
      "source": [
        "for text in d[d.grade=='12'].text_org:\n",
        "    hist, hypo = get_hist_hypo_references(text, nlp, nel, debug=True)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***NEW SENTENCE***\t\t  Well structured, impactful Corporate Social Investment (CSI) has the ability to contribute positively to nation building and drive positive change in the communities. MMI's commitment to social investment means that we are constantly looking for ways in which we can assist some of South Africa's most vulnerable citizens to expand their horizons and gain greater access to life's opportunities. This means that we do not view social investment as a nice to have or as an exercise in marketing or sponsorship but rather as a critical part of our contribution to society.\n",
            "\n",
            "Coreference Model Output\t [[\"some of South Africa 's most vulnerable citizens\", 'their']]\n",
            "NER + Pronouns Output\t\t ['we', 'we', 'their', 'we']\n",
            "Gender Guesser Output\t\t ['-', '-', '-', '-']\n",
            "NEL Output\t\t\t ['South Africa']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  HIV/AIDS is becoming a manageable disease in many developed countries but in a country such as ours, it remains a disease where people are still dying of this scourge unnecessarily. Metropolitan continues to make a difference in making sure that HIV AIDS moves away from being a death sentence to a manageable disease. Metropolitan's other focus area is education which remains the key to economic prosperity for our country.\n",
            "\n",
            "Coreference Model Output\t [['HIV / AIDS', 'it', 'this scourge', 'HIV AIDS'], ['Metropolitan', \"Metropolitan 's\"], ['ours', 'our country']]\n",
            "NER + Pronouns Output\t\t ['people']\n",
            "Gender Guesser Output\t\t ['-']\n",
            "NEL Output\t\t\t ['AIDS', 'AIDS']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  Momentum's focus on persons with disabilities ensures that this community is included and allowed to make their contribution to society. Orphaned and vulnerable children are another focus area for Momentum and projects supported ensure that children are allowed to grow up safely, to assume their role along with other children in inheriting a prosperous future.\n",
            "\n",
            "Coreference Model Output\t [['persons with disabilities', 'this community', 'their'], [\"Momentum 's\", 'Momentum'], ['children', 'their']]\n",
            "NER + Pronouns Output\t\t ['their', 'children', 'children', 'their', 'children']\n",
            "Gender Guesser Output\t\t ['-', '-', '-', '-', '-']\n",
            "NEL Output\t\t\t ['Momentum', 'Momentum']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  MOBILE & TABLET MOBI You can access this whole textbook on your mobile phone. Yes, the whole thing, anytime, anywhere. Visit the mobi sites at:  and   MXIT Don't stress if you haven't got a smart phone. All Mxit users can read their Everything Series textbooks on Mxit Reach too. Add Everything Maths and Everything Science as Mxit contacts or browse to the books on Mxit Reach. mxit>tradepost>reach>education> everything maths or everything science DOWNLOAD FOR TABLETS You can download a digital copy of the Everything Series textbooks for reading on your PC, tablet, iPad and Kindle.  and  PRACTISE INTELLIGENTLY PRACTISE FOR TESTS & EXAMS ONLINE & ON YOUR PHONE To do well in tests and exams you need practice, but knowing where to start and getting past exams papers can be difficult.\n",
            "\n",
            "Coreference Model Output\t [['this whole textbook', 'the whole thing'], ['MXIT', 'Mxit', 'Mxit', 'mxit'], ['All Mxit users', 'their'], ['Mxit Reach', 'Mxit Reach'], ['their Everything Series textbooks', 'the Everything Series textbooks']]\n",
            "NER + Pronouns Output\t\t ['You', 'you', 'their', 'You', 'you']\n",
            "Gender Guesser Output\t\t ['-', '-', '-', '-', '-']\n",
            "NEL Output\t\t\t ['MOBI', 'mobile phone', 'MXIT', 'smart phone', 'Mxit', 'Mxit', 'Mxit', 'Mxit', 'mxit', 'digital', 'iPad', 'Kindle']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  Intelligent Practice is an online Maths and Science practice service that allows you to practise questions at the right level of difficulty for you and get your answers checked instantly!\n",
            "\n",
            "Coreference Model Output\t []\n",
            "NER + Pronouns Output\t\t ['you', 'you']\n",
            "Gender Guesser Output\t\t ['-', '-']\n",
            "NEL Output\t\t\t []\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  YOUR DASHBOARD Your individualised dashboard on Intelligent Practice helps you keep track of your work. Your can check your progress and mastery for every topic in the book and use it to help you to manage your studies and target your weaknesses. You can also use your dashboard to show your teachers, parents, universities or bursary institutions what you have done during the year.\n",
            "\n",
            "Coreference Model Output\t [['Your individualised dashboard on Intelligent Practice', 'it', 'your dashboard']]\n",
            "NER + Pronouns Output\t\t ['you', 'you', 'You', 'you']\n",
            "Gender Guesser Output\t\t ['-', '-', '-', '-']\n",
            "NEL Output\t\t\t []\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  In this chapter you will learn how to gather evidence using the scientific method.\n",
            "\n",
            "Coreference Model Output\t []\n",
            "NER + Pronouns Output\t\t ['you']\n",
            "Gender Guesser Output\t\t ['-']\n",
            "NEL Output\t\t\t ['scientific method']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  The most important, and most exciting, thing about science and scientific theories is that they are not fixed. Hypotheses are formed and carefully tested, leading to scientific theories that explain those observations and predict results. The results are not made to fit the hypotheses. If new information comes to light with the use of better equipment, or the results of other experiments, this new information is used to improve and expand current theories. If a theory is found to have been incorrect it is changed to fit this new information. The data should never be made to fit the theory, if the data does not fit the theory then the theory is reworked or discarded. Although this changing of opinion is often taken for inconsistency, it is this very willingness to adapt that makes science useful, and allows new discoveries to be made. Remember that the term theory has a different meaning in science. A scientific theory is not like your theory of about why you can only ever find one sock. A scientific theory is one that has been tested and proven through repeated experiment and data. Scientists are constantly testing the data available, as well as commonly held beliefs, and it is this constant testing that allows progress, and improved theories.\n",
            "\n",
            "Coreference Model Output\t [['science and scientific theories', 'they'], ['Hypotheses', 'the hypotheses'], ['new information', 'this new information', 'this new information'], ['a theory', 'it', 'the theory', 'the theory', 'the theory'], ['The data', 'the data']]\n",
            "NER + Pronouns Output\t\t ['they', 'you']\n",
            "Gender Guesser Output\t\t ['-', '-']\n",
            "NEL Output\t\t\t ['scientific theory', 'scientific theory']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  The theory of gravity has been slowly developing since the beginning of the 16th century. Galileo Galilei is credited with some of the earliest work. At the time it was widely believed that heavier objects accelerated faster toward the earth than light objects did. Galileo had a hypothesis that this was not true, and performed experiments to prove this. Galileo's work allowed Sir Isaac Newton to hypothesise not only a theory of gravity on earth, but that gravity is what held the planets in their orbits. Newton's theory was used by John Couch Adams and Urbain Le Verrier to predict the planet Neptune in the solar system and this prediction was proved experimentally when Neptune was discovered by Johann Gottfried Galle.\n",
            "\n",
            "Coreference Model Output\t [['Galileo Galilei', 'Galileo', \"Galileo 's\"], ['accelerated', 'this', 'this'], ['the earth', 'earth'], ['the planets', 'their'], ['Sir Isaac Newton', \"Newton 's\"], ['predict', 'this prediction'], ['the planet Neptune in the solar system', 'Neptune']]\n",
            "NER + Pronouns Output\t\t ['Isaac Newton', 'John Couch Adams', 'Urbain Le Verrier', 'Johann Gottfried Galle', 'their']\n",
            "Gender Guesser Output\t\t ['-', '-', '-', '-', '-']\n",
            "NEL Output\t\t\t ['gravity', 'Galileo Galilei', 'Galileo', 'Galileo', 'Isaac Newton', 'gravity', 'gravity', 'Newton', 'John Couch Adams', 'Urbain Le Verrier', 'planet', 'Neptune', 'solar system', 'Neptune', 'Johann Gottfried Galle']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  Although a large majority of gravitational motion could be explained by Newton's theory of gravity, there were things that did not fit. But although a newer theory that better fits the facts was eventually proved by Albert Einstein, Newton's gravitational theory is still successfully used in many applications where the masses, speeds and energies are not too large.\n",
            "\n",
            "Coreference Model Output\t [[\"Newton 's\", \"Newton 's\"], [\"Newton 's theory of gravity\", \"Newton 's gravitational theory\"]]\n",
            "NER + Pronouns Output\t\t ['Albert Einstein']\n",
            "Gender Guesser Output\t\t ['-']\n",
            "NEL Output\t\t\t ['gravitational motion', 'Newton', 'gravity', 'Albert Einstein', 'Newton', 'gravitational']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  The principles of the three rules of thermodynamics describe how energy works, on all size levels (from the workings of the Earth's core, to a car engine). The basis for these three rules started as far back as 1650 with Otto von Guericke. He had a hypothesis that a vacuum pump could be made, and proved this by making one. In 1656 Robert Boyle and Robert Hooke used this information and built an air pump. Over the next 150 years the theory was expanded on and improved. Denis Papin built a steam pressuriser and release valve, and designed a piston cylinder and engine, which Thomas Savery and Thomas Newcomen built. These engines inspired the study of heat capacity and latent heat. Joseph Black and James Watt increased the steam engine efficiency and it was their work that Sadi Carnot (considered the father of thermodynamics) studied before publishing a discourse on heat, power, energy and engine efficiency in 1824. This work by Carnot was the beginning of modern thermodynamics as a science, with the first thermodynamics textbook written in 1859, and the first and second laws of thermodynamics being determined in the 1850s. Scientists such as Lord Kelvin, Max Planck, J. Willard Gibbs (all names you should recognise) among many many others studied thermodynamics. Over the course of 350 years thermodynamics has developed from the building of a vacuum pump, to some of the most important fundamental laws of energy.\n",
            "\n",
            "Coreference Model Output\t [['the three rules of thermodynamics', 'these three rules'], ['Otto von Guericke', 'He'], ['a hypothesis that a vacuum pump could be made', 'this', 'this information'], ['a piston cylinder and engine , which Thomas Savery and Thomas Newcomen built', 'These engines'], ['Joseph Black and James Watt', 'their'], ['a discourse on heat , power , energy and engine efficiency', 'This work by Carnot'], ['Sadi Carnot ( considered the father of thermodynamics )', 'Carnot']]\n",
            "NER + Pronouns Output\t\t ['Otto von Guericke', 'Robert Boyle', 'Robert Hooke', 'Denis Papin', 'Thomas Savery', 'Thomas Newcomen', 'Joseph Black', 'James Watt', 'Sadi Carnot', 'Carnot', 'Kelvin', 'Max Planck', 'J. Willard Gibbs', 'He', 'their', 'father', 'you']\n",
            "Gender Guesser Output\t\t ['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'M', '-', '-', 'M', '-', 'M', '-']\n",
            "NEL Output\t\t\t ['Earth', 'Otto von Guericke', 'vacuum pump', 'Robert Boyle', 'Robert Hooke', 'air pump', 'Denis Papin', 'valve', 'piston', 'Thomas Savery', 'Thomas Newcomen', 'heat capacity', 'latent heat', 'James Watt', 'steam engine', 'Carnot', 'Lord Kelvin', 'Max Planck', 'Willard', 'vacuum pump']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t [['Kelvin'], ['Otto von Guericke', 'He'], ['father']]\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  FACT Robert Boyle should be a familiar name to you. Boyle's law came about from his air pump experiments, where he discovered that pressure is inversely proportional to volume at a constant temperature (p 1 at constant T).\n",
            "\n",
            "Coreference Model Output\t [['Robert Boyle', 'his', 'he']]\n",
            "NER + Pronouns Output\t\t ['FACT Robert Boyle', 'Boyle', 'you', 'his', 'he']\n",
            "Gender Guesser Output\t\t ['-', '-', '-', 'M', 'M']\n",
            "NEL Output\t\t\t ['Robert Boyle']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t [['Robert Boyle', 'his', 'he']]\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  The scientific method is the basic skill process in the world of science. Since the beginning of time humans have been curious as to why and how things happen in the world around us. The scientific method provides scientists with a well structured scientific platform to help find the answers to their questions. Using the scientific method there is no limit as to what we can investigate. The scientific method can be summarised as follows: 1. Ask a question about the world around you. 2. Do background research on your questions. 3. Make a hypothesis about the event that gives a sensible result. You must be able to test your hypothesis through experiment. 4. Design an experiment to test the hypothesis. These methods must be repeatable and follow a logical approach. 5. Collect data accurately and interpret the data. You must be able to take measurements, collect information, and present your data in a useful format (drawings, explanations, tables and graphs). 6. Draw conclusions from the results of the experiment. Your observations must be made objectively, never force the data to fit your hypothesis. 7. Decide whether your hypothesis explains the data collected accurately. 8. If the data fits your hypothesis, verify your results by repeating the experiment or getting someone else to repeat the experiment. 9. If your data does not fit your hypothesis perform more background research and FACT In science we never 'prove' a hypothesis through a single experiment because there is a chance that you made an error somewhere along the way. What you can say is that your results SUPPORT the original hypothesis.\n",
            "\n",
            "Coreference Model Output\t [['humans', 'us'], ['The scientific method', 'The scientific method', 'the scientific method', 'The scientific method'], ['scientists', 'their'], ['a hypothesis about the event that gives a sensible result', 'your hypothesis', 'the hypothesis', 'the original hypothesis'], ['data', 'the data'], ['your hypothesis', 'your hypothesis'], ['the data', 'the data'], ['the experiment', 'the experiment'], ['your hypothesis', 'your hypothesis']]\n",
            "NER + Pronouns Output\t\t ['us', 'their', 'we', 'you', 'You', 'You', 'we', 'you', 'you']\n",
            "Gender Guesser Output\t\t ['-', '-', '-', '-', '-', '-', '-', '-', '-']\n",
            "NEL Output\t\t\t ['scientific method', 'scientific method', 'scientific method', 'scientific method', 'FACT']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  make a new hypothesis. Remember that in the development of both the gravitational theory and thermodynamics, scientists expanded on information from their predecessors or peers when developing their own theories. It is therefore very important to communicate findings to the public in the form of scientific publications, at conferences, in articles or TV or radio programmes. It is important to present your experimental data in a specific format, so that others can read your work, understand it, and repeat the experiment. 1. Aim: A brief sentence describing the purpose of the experiment. 2. Apparatus: A list of the apparatus. 3. Method: A list of the steps followed to carry out the experiment. 4. Results: Tables, graphs and observations about the experiment. 5. Discussion: What your results mean. 6. Conclusion: A brief sentence concluding whether or not the aim was met. A hypothesis A hypothesis should be specific and should relate directly to the question you are asking. For example if your question about the world was, why do rainbows form, your hypothesis could be: Rainbows form because of light shining through water droplets. After formulating a hypothesis, it needs to be tested through experiment. An incorrect prediction does not mean that you have failed. It means that the experiment has brought some new facts to light that you might not have thought of before.\n",
            "\n",
            "Coreference Model Output\t [['scientists', 'their', 'their'], ['your work', 'it'], ['the experiment', 'the experiment', 'the experiment'], ['rainbows', 'Rainbows'], ['a hypothesis', 'it'], ['An incorrect prediction', 'It']]\n",
            "NER + Pronouns Output\t\t ['their', 'their', 'you', 'you', 'you']\n",
            "Gender Guesser Output\t\t ['-', '-', '-', '-', '-']\n",
            "NEL Output\t\t\t ['gravitational', 'radio']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  Activity: Designing your own experiment Recording and writing up an investigation is an integral part of the scientific method. In this activity you are required to design your own experiment. Use the information provided below, and the flow diagram in the previous experiment to help you design your experiment. The experiment should be handed in as a 1 2 page report. Below are basic steps to follow when designing your own experiment. 1. Ask a question which you want to find an answer to. 2. Perform background research on your topic of choice. 3. Write down your hypothesis. 4. Identify variables important to your investigation: those that are relevant, those you can measure or observe. 5. Decide on the independent and dependent variables in your experiment, and those variables that must be kept constant. 6. Design the experiment you will use to test your hypothesis: State the aim of the experiment. List the apparatus (equipment) you will need to perform the experiment. Write the method that will be used to test your hypothesis in bullet format in the correct sequence, with each step of the experiment numbered. Indicate how the results should be presented, and what data is required.\n",
            "\n",
            "Coreference Model Output\t [['your own experiment', 'your experiment', 'The experiment'], ['your hypothesis', 'your hypothesis', 'your hypothesis'], ['the experiment you will use to test your hypothesis', 'the experiment', 'the experiment', 'the experiment']]\n",
            "NER + Pronouns Output\t\t ['you', 'you', 'you', 'you', 'you', 'you']\n",
            "Gender Guesser Output\t\t ['-', '-', '-', '-', '-', '-']\n",
            "NEL Output\t\t\t ['integral', 'scientific method', 'The experiment', 'bullet']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  Before you perform an experiment you should be comfortable with certain apparatus that you will be using. The following pages give some commonly used apparatus and how to use them.\n",
            "\n",
            "Coreference Model Output\t [['some commonly used apparatus', 'them']]\n",
            "NER + Pronouns Output\t\t ['you', 'you', 'you', 'them']\n",
            "Gender Guesser Output\t\t ['-', '-', '-', '-']\n",
            "NEL Output\t\t\t []\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  with numbers next to them mark off every 10 mm (1 cm).\n",
            "\n",
            "Coreference Model Output\t []\n",
            "NER + Pronouns Output\t\t ['them']\n",
            "Gender Guesser Output\t\t ['-']\n",
            "NEL Output\t\t\t []\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  A burette is used to accurately measure the volume of a liquid added in an experiment. The valve at the bottom allows the liquid to be added drop-by-drop, and the initial and final volume can be measured so that the total volume added is known. More information about burettes is given to you in your first titration experiment this year in Chapter 9.\n",
            "\n",
            "Coreference Model Output\t [['a liquid added in an experiment', 'the liquid']]\n",
            "NER + Pronouns Output\t\t ['you']\n",
            "Gender Guesser Output\t\t ['-']\n",
            "NEL Output\t\t\t ['burette', 'valve', 'titration']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  There are two types of pipettes you might encounter this year. A volumetric pipette has a large bulb, marked with the set volume it can measure. Above the bulb on these pipettes there is a line. For a 5 ml volumetric pipette, when the meniscus of your liquid sits on the line, then the volume in that pipette is 5 ml.\n",
            "\n",
            "Coreference Model Output\t [['A volumetric pipette', 'it', 'these pipettes'], ['a large bulb , marked with the set volume it can measure', 'the bulb on these pipettes'], ['a line', 'the line'], ['a 5 ml volumetric pipette', 'that pipette']]\n",
            "NER + Pronouns Output\t\t ['you']\n",
            "Gender Guesser Output\t\t ['-']\n",
            "NEL Output\t\t\t ['volumetric pipette', 'bulb', 'bulb', 'volumetric pipette', 'pipette']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  A graduated pipette has the same type of marking you see on a burette. The top is 0 ml, and the volume increases as you move down the pipette. In this pipette you should fill the pipette to near the 0 ml line and make a note of the volume. You can then add the desired volume, stopping when the volume in the pipette has decreased by the required amount.\n",
            "\n",
            "Coreference Model Output\t [['A graduated pipette', 'this pipette'], ['the pipette', 'the pipette', 'the pipette']]\n",
            "NER + Pronouns Output\t\t ['you', 'you', 'you', 'You']\n",
            "Gender Guesser Output\t\t ['-', '-', '-', '-']\n",
            "NEL Output\t\t\t ['graduated pipette', 'burette', 'pipette', 'pipette', 'pipette', 'pipette']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  A learner wondered whether the rate of evaporation of a substance was related to the boiling point of the substance. Having done background research they realised that the boiling point of a substance is linked to the intermolecular forces within the substance. They know that greater intermolecular forces require more energy to overcome. This led them to form the following hypothesis: The larger the intermolecular forces of a substance the higher the boiling point. Therefore, if a substance has higher boiling point it will have a slower rate of evaporation. Perform the following experiment that the learner designed to test that hypothesis.\n",
            "\n",
            "Coreference Model Output\t [['a substance', 'the substance'], ['A learner', 'they', 'They', 'them', 'the learner'], ['a substance', 'the substance'], ['require', 'This'], ['the intermolecular forces within the substance', 'the intermolecular forces of a substance'], ['a substance', 'it'], ['the following hypothesis', 'that hypothesis']]\n",
            "NER + Pronouns Output\t\t ['they', 'They', 'them']\n",
            "Gender Guesser Output\t\t ['-', '-', '-']\n",
            "NEL Output\t\t\t ['evaporation', 'boiling point', 'boiling point', 'intermolecular forces', 'intermolecular forces', 'energy', 'intermolecular forces', 'boiling point', 'boiling point', 'evaporation']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  Experiment: Boiling points and rate of evaporation: Part 1 Aim: To determine whether the rate of evaporation of a substance is related to its boiling point. Apparatus: You will need the following items for this experiment: 220 ml water, 20 ml methylated spirits, 20 ml nail polish remover, 20 ml water, 20 ml ethanol One 250 ml beaker, four 20 ml beakers, a thermometer, a stopwatch or clock Method: WARNING!\n",
            "\n",
            "Coreference Model Output\t [['a substance', 'its'], ['Part 1', 'this experiment']]\n",
            "NER + Pronouns Output\t\t ['You']\n",
            "Gender Guesser Output\t\t ['-']\n",
            "NEL Output\t\t\t ['evaporation', 'evaporation', 'boiling point', 'methylated spirits', 'nail polish remover', 'ethanol', 'thermometer', 'stopwatch']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n",
            "***NEW SENTENCE***\t\t  Remember that without units much of our work as scientists would be meaningless. We need to express our thoughts clearly and units give meaning to the numbers we measure and calculate. Depending on which units we use, the numbers are different. For example if you have 12 water, it means nothing. You could have 12 ml of water, 12 litres of water, or even 12 bottles of water. Units are an essential part of the language we use. Units must be specified when expressing physical quantities.\n",
            "\n",
            "Coreference Model Output\t [['the numbers we measure and calculate', 'the numbers'], ['12 water', 'it']]\n",
            "NER + Pronouns Output\t\t ['We', 'we', 'we', 'you', 'You', 'we']\n",
            "Gender Guesser Output\t\t ['-', '-', '-', '-', '-', '-']\n",
            "NEL Output\t\t\t ['physical quantities']\n",
            "\n",
            "Historical references\t\t []\n",
            "Hypothetical references\t\t []\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-160-814169425112>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrade\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'12'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_org\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hist_hypo_references\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-158-c9c2ab4d5b46>\u001b[0m in \u001b[0;36mget_hist_hypo_references\u001b[0;34m(text, nlp, nel, debug)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_hist_hypo_references\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallen_predictor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mperson_mentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mget_pronouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_gender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mperson_mentions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-149-f4b8902fda05>\u001b[0m in \u001b[0;36mget_coref\u001b[0;34m(text, predictor)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_coref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clusters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp_models/coref/predictors/coref.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mrepresentation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mcoreference\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \"\"\"\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"document\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_tokenized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_document\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mpredict_json\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json_to_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjson_to_labeled_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mpredict_instance\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstance\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_token_indexers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_on_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mforward_on_instance\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensors\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minto\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mremove\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \"\"\"\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_on_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_on_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mforward_on_instances\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_output_human_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             instance_separated_output: List[Dict[str, numpy.ndarray]] = [\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp_models/coref/models/coref.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, spans, span_labels, metadata)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# Shape: (batch_size, num_spans)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         span_mention_scores = self._mention_scorer(\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mention_feedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         ).squeeze(-1)\n\u001b[1;32m    220\u001b[0m         \u001b[0;31m# Shape: (batch_size, num_spans) for all 3 tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/modules/time_distributed.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pass_through, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mreshaped_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mreshaped_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mreshaped_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mreshaped_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msome_input\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/modules/feedforward.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_linear_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         ):\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhwSyqxsz_LB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}