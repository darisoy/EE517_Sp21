{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "EE517: Project_Combine.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darisoy/EE517_Sp21/blob/master/Project/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eQCgMJ3HVCz"
      },
      "source": [
        "# Set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mek_yYjHDLG"
      },
      "source": [
        "# install packages\n",
        "%%capture\n",
        "!pip install allennlp\n",
        "!pip install allennlp-models\n",
        "!pip install spacy-dbpedia-spotlight\n",
        "\n",
        "import spacy\n",
        "from allennlp.predictors.predictor import Predictor\n",
        "import spacy_dbpedia_spotlight"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPj_y-xBUnUx"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcGLEDtMVHJ0"
      },
      "source": [
        "def get_coref(text, dic, predictor):\n",
        "    prediction = predictor.predict(document=text)\n",
        "    return [[dic.index(token) for token in cluster] for cluster in prediction['clusters']]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGGuD-hCVxaI"
      },
      "source": [
        "def get_ner(text, dic, nlp):\n",
        "    doc = nlp(text)\n",
        "    return [dic.index([ent.start, ent.end-1]) for ent in doc.ents if ent.label_ == 'PERSON']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eNSPah3ZQr6"
      },
      "source": [
        "def get_nel(text, dic, nel):\n",
        "    threshold = 0.95\n",
        "    doc = nel(text)\n",
        "    return [dic.index([ent.start, ent.end-1]) for ent in doc.ents if float(ent._.dbpedia_raw_result['@similarityScore']) >= threshold]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efp5gTG62iN5"
      },
      "source": [
        "def id_to_string(id, dic, doc):\n",
        "    [a, b] = dic[id]\n",
        "    return doc[a:b+1].text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9tqzGuh819J"
      },
      "source": [
        "def get_cluster(person, clusters):\n",
        "    for i, cluster in enumerate(clusters):\n",
        "        if person in cluster:\n",
        "            return clusters[i]\n",
        "    return [person]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDhS0Ov5faHy"
      },
      "source": [
        "def split_hist_hypo(clusters, person_mentions, famous_people):\n",
        "    historical = []\n",
        "    hypothetical = []\n",
        "    for person in person_mentions:\n",
        "        in_historical = any(person in sublist for sublist in historical)\n",
        "        in_hypothetical = any(person in sublist for sublist in hypothetical)\n",
        "        if in_historical or in_hypothetical:\n",
        "            continue\n",
        "        person_set = get_cluster(person, clusters)\n",
        "        if person in famous_people:\n",
        "            historical.append(person_set)\n",
        "        else:\n",
        "            hypothetical.append(person_set)\n",
        "    return historical, hypothetical"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hEl8Gpm-SFZ"
      },
      "source": [
        "def array_to_text(array2D, dic, doc):\n",
        "    return [[id_to_string(e, dic, doc) for e in arr] for arr in array2D]"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dIb0xPzZcZK"
      },
      "source": [
        "def get_hist_hypo_references(text, nlp, nel, debug=False):\n",
        "    doc = nlp(text)\n",
        "    dic = [[ent.start, ent.end-1] for ent in list(doc.noun_chunks)]\n",
        "    clusters = get_coref(text, dic, allen_predictor)\n",
        "    person_mentions = get_ner(text, dic, nlp)\n",
        "    famous_people = get_nel(text, dic, nel)\n",
        "    # TODO: get non-name person mentions using simple rules (for he, she, we, you...)\n",
        "    # person_mentions.append(...)\n",
        "    hist, hypo = split_hist_hypo(clusters, person_mentions, famous_people)\n",
        "    hist = array_to_text(hist, dic, doc)\n",
        "    hypo = array_to_text(hypo, dic, doc)\n",
        "    if debug:\n",
        "        print('Sentence:')\n",
        "        print(text)\n",
        "        print()\n",
        "        print('Coreferences:')\n",
        "        print(array_to_text(clusters, dic, doc))\n",
        "        print('People:')\n",
        "        print(array_to_text([person_mentions], dic, doc))\n",
        "        print('Famous:')\n",
        "        print(array_to_text([famous_people], dic, doc))\n",
        "        print()\n",
        "        print('Historical references:')\n",
        "        print(hist)\n",
        "        print('Hypothetical references:')\n",
        "        print(hypo)\n",
        "    return hist, hypo"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cXTj74F_1Ll"
      },
      "source": [
        "# Get models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwffFN3Icgrf"
      },
      "source": [
        "%%capture\n",
        "!python -m spacy download en_core_web_lg\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "allen_model_url = 'https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz'\n",
        "allen_predictor = Predictor.from_path(allen_model_url)  # load the model\n",
        "nel = spacy_dbpedia_spotlight.create('en')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9B1orA3G6NF"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehhkjavYGO5r",
        "outputId": "3402b684-ff50-4a3c-f809-3bbe1c153363"
      },
      "source": [
        "sample = 'Isaac Newton invented the wheel. He didn\\'t go to kindergarden but he was familiar with circles. When told this story, Jessica didn\\'t believe it. She thought Newton was a lie.'\n",
        "hist, hypo = get_hist_hypo_references(sample, nlp, nel, debug=True)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence:\n",
            "Isaac Newton invented the wheel. He didn't go to kindergarden but he was familiar with circles. When told this story, Jessica didn't believe it. She thought Newton was a lie.\n",
            "\n",
            "Coreferences:\n",
            "[['Isaac Newton', 'He', 'he', 'Newton'], ['this story', 'it'], ['Jessica', 'She']]\n",
            "People:\n",
            "[['Isaac Newton', 'Jessica', 'Newton']]\n",
            "Famous:\n",
            "[['Isaac Newton', 'Newton']]\n",
            "\n",
            "Historical references:\n",
            "[['Isaac Newton', 'He', 'he', 'Newton']]\n",
            "Hypothetical references:\n",
            "[['Jessica', 'She']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB2dutoaHroM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}